{"cells":[{"cell_type":"code","execution_count":null,"id":"097f16f8","metadata":{"id":"097f16f8"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from sklearn.preprocessing import MinMaxScaler\n","from torch.utils.data import DataLoader, TensorDataset\n","import numpy as np\n","from geopy.geocoders import Nominatim\n","import math\n","import statistics\n","import pandas as pd\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","current_directory_path = '/content/drive/MyDrive/AML' # modify with your path if needed!!!!!\n","\n","data_path = current_directory_path + '/data'\n","model_price_path = current_directory_path + '/models/model_price.pt'"],"metadata":{"id":"o2jCAzPSpnBI"},"id":"o2jCAzPSpnBI","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"e0ef2bf9","metadata":{"id":"e0ef2bf9"},"source":["### Data processing:"]},{"cell_type":"code","execution_count":null,"id":"e1dfce2a","metadata":{"id":"e1dfce2a"},"outputs":[],"source":["#Countries_test = {}\n","#for index,data in df.iterrows():\n","#    num_of_null = 0\n","#    avg = 0\n","#    count = 0\n","#    nums = []\n","#    for i in range(1985,2010):\n","#        value = data[str(i)]\n","#        if type(value) == str:\n","#            value = float(value.replace(\",\",\"\"))\n","#        if  math.isnan(value):\n","#            num_of_null += 1\n","#        else:\n","#            avg += value\n","#            count += 1\n","#            nums.append(value)\n","\n","#    Countries_test[data[\"Country\"]] = {\"avg\": round(avg / count,2) if count != 0 else math.nan,\n","#                                       \"null%\": round((num_of_null / (num_of_null + count)) * 100,2),\n","#                                       \"mean\": round(statistics.mean(nums),2)\n","#                                       }\n","#Countries_test"]},{"cell_type":"code","execution_count":null,"id":"8cd4c8c0","metadata":{"id":"8cd4c8c0"},"outputs":[],"source":["def getCountries():\n","    df = pd.read_excel(data_path+\"/History_2.xlsx\")\n","    Countries = {}\n","\n","    for index,data in df.iterrows():\n","        longest_chain = []\n","        current_longest = []\n","        start_year = 1985\n","        for i in range(1985,2010):\n","            value = data[str(i)]\n","            if type(value) == str:\n","                value = float(value.replace(\",\",\"\"))\n","\n","            if  math.isnan(value):\n","                start_year = start_year if len(current_longest) > len(longest_chain) else i - len(longest_chain)\n","                current_longest = current_longest if len(current_longest) > len(longest_chain) else longest_chain\n","                longest_chain = []\n","            else:\n","                longest_chain.append(value)\n","\n","        start_year = start_year if len(current_longest) > len(longest_chain) else i - len(longest_chain) + 1\n","        current_longest = current_longest if len(current_longest) > len(longest_chain) else longest_chain\n","        Countries[data[\"Country\"]] = (current_longest,start_year)\n","\n","    return Countries\n","\n","Countries = getCountries()"]},{"cell_type":"code","execution_count":null,"id":"1022ea09","metadata":{"id":"1022ea09"},"outputs":[],"source":["def toTensor(country_dataset, seq_length=3):\n","    all_x, all_y,scalers = [], [], []\n","\n","    for name, data in country_dataset.items():\n","        seq = data[0]\n","        if len(seq) <= seq_length:\n","            continue\n","\n","        scaler = MinMaxScaler()                                             # chatted with GPT for scaling down the data\n","        scaled = scaler.fit_transform(np.array(seq).reshape(-1,1)).flatten()\n","\n","        for i in range(len(scaled) - seq_length):\n","            prev = scaled[i:i+seq_length]\n","            target = scaled[i+seq_length]\n","\n","            all_x.append(prev)\n","            all_y.append(target)\n","            scalers.append(scaler)\n","\n","    return torch.tensor(np.array(all_x), dtype=torch.float32).unsqueeze(-1), torch.tensor(np.array(all_y), dtype=torch.float32), scalers"]},{"cell_type":"code","execution_count":null,"id":"ac5328e1","metadata":{"id":"ac5328e1"},"outputs":[],"source":["def getData():\n","    countries = getCountries()\n","    x,y,scalers = toTensor(countries)\n","    train_size = int(len(x) * 0.67)\n","\n","    train_x = x[:train_size]\n","    test_x = x[train_size:]\n","    train_y = y[:train_size]\n","    test_y = y[train_size:]\n","    train_s = scalers[:train_size]\n","    test_s = scalers[train_size:]\n","\n","    return train_x,train_y,train_s,test_x,test_y,test_s"]},{"cell_type":"code","execution_count":null,"id":"4ae20353","metadata":{"id":"4ae20353"},"outputs":[],"source":["def setDevice():\n","    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","device = setDevice()"]},{"cell_type":"markdown","id":"3c8397df","metadata":{"id":"3c8397df"},"source":["### Model:"]},{"cell_type":"code","execution_count":null,"id":"887caa2c","metadata":{"id":"887caa2c"},"outputs":[],"source":["#used code provided/created in the labs surounding LSTM\n","class LSTMModel(nn.Module):\n","    def __init__(self, input_size, hidden_dim, num_layers=2):\n","        super(LSTMModel, self).__init__()\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.hidden_size = hidden_dim\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_dim,num_layers,batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n","\n","        out, _ = self.lstm(x, (h0,c0))\n","        out = self.fc(out[:,-1,:])\n","        return out"]},{"cell_type":"markdown","id":"3d6fa6d4","metadata":{"id":"3d6fa6d4"},"source":["### Train the model:"]},{"cell_type":"code","execution_count":null,"id":"bb72dbdd","metadata":{"id":"bb72dbdd"},"outputs":[],"source":["# https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n","# used for insperation\n","\n","def train_and_evaluate():\n","    train_x,train_y,train_s,test_x,test_y,test_s = getData()\n","    LR = 0.0001\n","    num_epochs = 2000\n","    train_ds = TensorDataset(train_x.float(), train_y.float())\n","    test_ds = TensorDataset(test_x.float(), test_y.float())\n","\n","    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n","    test_loader = DataLoader(test_ds, batch_size=8)\n","\n","    model = LSTMModel(1, 128).to(device)\n","    criterion = nn.MSELoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n","\n","    losses = []\n","    val_losses = []\n","    for epoch in range(num_epochs): # used code provided in the lab\n","        model.train()\n","        total_loss = 0\n","        for xb, yb in train_loader:\n","            xb = xb.to(device)\n","            yb = yb.view(-1, 1).to(device)\n","            pred = model(xb)\n","            loss = criterion(pred, yb)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","            optimizer.step()\n","            total_loss += loss.item()\n","        losses.append(total_loss)\n","\n","        model.eval()\n","        val_loss = 0\n","        with torch.no_grad():\n","            for xb, yb in test_loader:\n","                xb = xb.to(device)\n","                yb = yb.view(-1, 1).to(device)\n","                pred = model(xb)\n","                loss = criterion(pred, yb)\n","                val_loss += loss.item()\n","        val_losses.append(val_loss)\n","        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}, Val_loss: {val_loss:.4f}\")\n","\n","    return model, losses, val_losses\n","\n","# model, losses, val_losses = train_and_evaluate()\n"]},{"cell_type":"code","execution_count":null,"id":"787a8fdc","metadata":{"id":"787a8fdc"},"outputs":[],"source":["def export_model(model):\n","    model_price = torch.jit.script(model)\n","    model_price.save(model_price_path)\n","\n","# export_model(model)"]},{"cell_type":"code","execution_count":null,"id":"e218be06","metadata":{"id":"e218be06"},"outputs":[],"source":["#def visualize_losses():\n","#    model, losses,val_losses = train_and_evaluate()\n","#    plt.plot(losses, label=\"training loss\")\n","#    plt.plot(val_losses, label=\"validation loss\")\n","#    plt.legend()\n","#    plt.show()\n"]},{"cell_type":"markdown","id":"feb6ebd2","metadata":{"id":"feb6ebd2"},"source":["### Predict the Price:"]},{"cell_type":"code","execution_count":null,"id":"20645bd9","metadata":{"id":"20645bd9"},"outputs":[],"source":["def toTensorPred(dataset, seq_length=3):\n","    if len(dataset) < seq_length:\n","        return False\n","\n","    scaler = MinMaxScaler()\n","    scaled = scaler.fit_transform(np.array(dataset).reshape(-1,1)).flatten()\n","\n","    return scaled[-seq_length:], scaler\n","\n"]},{"cell_type":"code","execution_count":null,"id":"60e2e644","metadata":{"id":"60e2e644"},"outputs":[],"source":["def inflation_value(i):\n","    return math.pow((1+0.03), i)\n","\n","def predict_end_year(input_sequence,start_year, end_year):\n","    start_year_a = start_year + len(input_sequence) - 1\n","    inp_seq, sc = toTensorPred(input_sequence)\n","    model = torch.jit.load(model_price_path)\n","\n","    values = []\n","\n","    for i in range(end_year - start_year_a):\n","        inp = torch.tensor(inp_seq, dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device) # used gpt due to not understanding unsqueezed\n","        with torch.no_grad():\n","            pred = model(inp)\n","        pred_value = pred.item()\n","        inf = inflation_value(i)\n","        values.append(sc.inverse_transform([[pred_value]])[0][0] * inf)\n","        inp_seq = np.append(inp_seq[1:],pred_value)\n","\n","    return values\n"]},{"cell_type":"code","execution_count":null,"id":"7de16f4d","metadata":{"id":"7de16f4d"},"outputs":[],"source":["#print(Countries[\"Denmark\"][1])\n","#print(\"prediction with accounted inflation: \" + str(test[-1]))\n","#print(\"goals: 21,421\")\n","#plt.plot(Countries[\"Denmark\"][0], label='Original')\n","#plt.plot(range(len(Countries[\"Denmark\"][0]), len(Countries[\"Denmark\"][0]) + len(test)), test, label='Forecast', color='red')\n","#plt.show()"]},{"cell_type":"code","execution_count":null,"id":"aa4ccc92","metadata":{"id":"aa4ccc92","outputId":"3aae9604-0f76-4361-f5bc-be255259d77f"},"outputs":[{"data":{"text/plain":["25430.46"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# https://www.geeksforgeeks.org/get-the-city-state-and-country-names-from-latitude-and-longitude-using-python/\n","# used for getting the country from a location\n","\n","def predict_end_year_from_cords(lat, long, end_year):\n","    geolocator = Nominatim(user_agent=\"my_geopy_app\")\n","\n","    location = geolocator.reverse(str(lat)+\",\"+str(long), language='en')\n","\n","    address = location.raw['address']\n","    country = address['country']\n","\n","    if(country in Countries.keys()):\n","        return round(predict_end_year(Countries[country][0],Countries[country][1],end_year)[-1],2)\n","    else:\n","        avg_pred = []\n","        for c in Countries.keys():\n","            avg_pred.append(predict_end_year(Countries[c][0],Countries[c][1],end_year)[-1])\n","        return round(statistics.mean(avg_pred),2)\n","\n","\n","# predict_end_year_from_cords(-33.9248685,18.4240553,2024)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}